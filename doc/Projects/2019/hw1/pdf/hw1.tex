%%
%% Automatically generated file from DocOnce source
%% (https://github.com/hplgit/doconce/)
%%
%%


%-------------------- begin preamble ----------------------

\documentclass[%
oneside,                 % oneside: electronic viewing, twoside: printing
final,                   % draft: marks overfull hboxes, figures with paths
10pt]{article}

\listfiles               %  print all files needed to compile this document

\usepackage{relsize,makeidx,color,setspace,amsmath,amsfonts,amssymb}
\usepackage[table]{xcolor}
\usepackage{bm,ltablex,microtype}

\usepackage[pdftex]{graphicx}

\usepackage{fancyvrb} % packages needed for verbatim environments

\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}

\usepackage{lmodern}         % Latin Modern fonts derived from Computer Modern

% Hyperlinks in PDF:
\definecolor{linkcolor}{rgb}{0,0,0.4}
\usepackage{hyperref}
\hypersetup{
    breaklinks=true,
    colorlinks=true,
    linkcolor=linkcolor,
    urlcolor=linkcolor,
    citecolor=black,
    filecolor=black,
    %filecolor=blue,
    pdfmenubar=true,
    pdftoolbar=true,
    bookmarksdepth=3   % Uncomment (and tweak) for PDF bookmarks with more levels than the TOC
    }
%\hyperbaseurl{}   % hyperlinks are relative to this root

\setcounter{tocdepth}{2}  % levels in table of contents

% --- fancyhdr package for fancy headers ---
\usepackage{fancyhdr}
\fancyhf{} % sets both header and footer to nothing
\renewcommand{\headrulewidth}{0pt}
\fancyfoot[LE,RO]{\thepage}
% Ensure copyright on titlepage (article style) and chapter pages (book style)
\fancypagestyle{plain}{
  \fancyhf{}
  \fancyfoot[C]{{\footnotesize \copyright\ 1999-2019, "Data Analysis and Machine Learning FYS-STK3155/FYS4155":"http://www.uio.no/studier/emner/matnat/fys/FYS3155/index-eng.html". Released under CC Attribution-NonCommercial 4.0 license}}
%  \renewcommand{\footrulewidth}{0mm}
  \renewcommand{\headrulewidth}{0mm}
}
% Ensure copyright on titlepages with \thispagestyle{empty}
\fancypagestyle{empty}{
  \fancyhf{}
  \fancyfoot[C]{{\footnotesize \copyright\ 1999-2019, "Data Analysis and Machine Learning FYS-STK3155/FYS4155":"http://www.uio.no/studier/emner/matnat/fys/FYS3155/index-eng.html". Released under CC Attribution-NonCommercial 4.0 license}}
  \renewcommand{\footrulewidth}{0mm}
  \renewcommand{\headrulewidth}{0mm}
}

\pagestyle{fancy}


% prevent orhpans and widows
\clubpenalty = 10000
\widowpenalty = 10000

% --- end of standard preamble for documents ---


% insert custom LaTeX commands...

\raggedbottom
\makeindex
\usepackage[totoc]{idxlayout}   % for index in the toc
\usepackage[nottoc]{tocbibind}  % for references/bibliography in the toc

%-------------------- end preamble ----------------------

\begin{document}

% matching end for #ifdef PREAMBLE

\newcommand{\exercisesection}[1]{\subsection*{#1}}


% ------------------- main content ----------------------



% ----------------- title -------------------------

\thispagestyle{empty}

\begin{center}
{\LARGE\bf
\begin{spacing}{1.25}
Homework 1
\end{spacing}
}
\end{center}

% ----------------- author(s) -------------------------

\begin{center}
{\bf \href{{http://www.uio.no/studier/emner/matnat/fys/FYS3155/index-eng.html}}{Data Analysis and Machine Learning FYS-STK3155/FYS4155}}
\end{center}

    \begin{center}
% List of all institutions:
\centerline{{\small Department of Physics, University of Oslo, Norway}}
\end{center}
    
% ----------------- end author(s) -------------------------

% --- begin date ---
\begin{center}
Jan 21, 2019
\end{center}
% --- end date ---

\vspace{1cm}


\subsection*{Exercise 1}

The first exercise here is of a mere technical art. We want you to have 
\begin{itemize}
\item git as a version control software and to establish a user account on a provider like GitHub. Other providers like GitLab etc are equally fine. You can also use the University of Oslo \href{{https://www.uio.no/tjenester/it/maskin/filer/versjonskontroll/github.html}}{GitHub facilities}. 

\item Install various Python packages
\end{itemize}

\noindent
We will make extensive use of Python as programming language and its
myriad of available libraries.  You will find
IPython/Jupyter notebooks invaluable in your work.  You can run \textbf{R}
codes in the Jupyter/IPython notebooks, with the immediate benefit of
visualizing your data. You can also use compiled languages like C++,
Rust, Fortran etc if you prefer. The focus in these lectures will be
on Python.

If you have Python installed (we recommend Python3.6 or higher versions) and you feel
pretty familiar with installing different packages, we recommend that
you install the following Python packages via \textbf{pip} as 

\begin{enumerate}
\item pip install numpy scipy matplotlib ipython scikit-learn sympy pandas pillow 
\end{enumerate}

\noindent
For \textbf{Tensorflow}, we recommend following the instructions in the text of 
\href{{http://shop.oreilly.com/product/0636920052289.do}}{Aurelien Geron, Hands‑On Machine Learning with Scikit‑Learn and TensorFlow, O'Reilly}

We will come back to \textbf{tensorflow} later. 

For Python3, replace \textbf{pip} with \textbf{pip3}.

For OSX users we recommend, after having installed Xcode, to
install \textbf{brew}. Brew allows for a seamless installation of additional
software via for example 

\begin{enumerate}
\item brew install python3
\end{enumerate}

\noindent
For Linux users, with its variety of distributions like for example the widely popular Ubuntu distribution,
you can use \textbf{pip} as well and simply install Python as 

\begin{enumerate}
\item sudo apt-get install python3  (or python for pyhton2.7)
\end{enumerate}

\noindent
If you don't want to perform these operations separately and venture
into the hassle of exploring how to set up dependencies and paths, we
recommend two widely used distrubutions which set up all relevant
dependencies for Python, namely 

\begin{itemize}
\item \href{{https://docs.anaconda.com/}}{Anaconda}, 
\end{itemize}

\noindent
which is an open source
distribution of the Python and R programming languages for large-scale
data processing, predictive analytics, and scientific computing, that
aims to simplify package management and deployment. Package versions
are managed by the package management system \textbf{conda}. 

\begin{itemize}
\item \href{{https://www.enthought.com/product/canopy/}}{Enthought canopy} 
\end{itemize}

\noindent
is a Python
distribution for scientific and analytic computing distribution and
analysis environment, available for free and under a commercial
license.

We recommend using \textbf{Anaconda}.


\subsection*{Exercise 2}

You should install and explore
\begin{enumerate}
\item Numpy and Scipy

\item Matplotlib

\item Pandas

\item Jupyter notebook
\end{enumerate}

\noindent
\paragraph{Simple Random walk.}
Make then a simple program which simulates a random walk and then plots the first 100 values. The example here may help.
\begin{print}
import random
import matplotlib.pyplot as plt
position = 0  
steps = 1000
for i in range(steps):     
    step = 1 if random.randint(0,1) else -1
    position+= step
    walk.append(position)

plt.plot(walk[:100])
plt.show()
\end{print}

\paragraph{Simple Linear algebra example.}
Write a simple program which performs basic matrix-vector multiplications and finds also the inverse of a matrix. You could use the following example
\begin{print}
import numpy as np
from numpy.linalg import inv
x = np.array([[1., 2., 3.],[4.,5.,6.]])
y = np.array([[6.,23,],[-1.,7.],[8.,9.]])
print(x)
print(y)
z = x.dot(y)
# equivalent to np.dot(x,y)
print(z)
print(np.dot(x,y))
z = np.dot(x,np.ones(3))
# or write it as 
z = x @ np.ones(3)
print(z)

X = np.random.randn(5,5)
mat = X.T.dot(X)
print(inv(mat))
print(mat.dot(inv(mat)))
\end{print}
\subsection*{Exercise 3}

We will generate our own dataset for a function $y(x)$ where $x \in [0,1]$ and defined by random numbers computed with the uniform distribution. The function $y$ is a quadratic polynomial in $x$ with added stochastic noise according to the normal distribution $\cal {N}(0,1)$.
The following simple Python instructions define our $x$ and $y$ values (with 100 data points).
\begin{print}
x = np.random.rand(100,1)
y = 5*x*x+0.1*np.random.randn(100,1)
\end{print}

\begin{enumerate}
\item Write your own code (following the examples under the \href{{https://compphysics.github.io/MachineLearning/doc/pub/Regression/html/Regression-bs.html}}{regression slides}) for computing the parametrization of the data set fitting a second-order polynomial. 

\item Use thereafter \textbf{scikit-learn} (see again the examples in the regression slides) and compare with your own code.   

\item Using scikit-learn, compute also the mean square error, a risk metric corresponding to the expected value of the squared (quadratic) error defined as
\end{enumerate}

\noindent
\[ MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2, 
\] 
and the $R^2$ score function.
If $\tilde{\hat{y}}_i$ is the predicted value of the $i-th$ sample and $y_i$ is the corresponding true value, then the score $R^2$ is defined as
\[
R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
\]
where we have defined the mean value  of $\hat{y}$ as
\[
\bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
\]
You can use the functionality included in scikit-learn. If you feel for it, you can use your own program and define functions which compute the above two functions. 
Discuss the meaning of these results. Try also to vary the coefficient in front of the added stochastic noise term and discuss the quality of the fits.




\subsection*{Exercise 4, variance of the parameters $\beta$ in linear regression}

Show that the variance of the parameters $\beta$ in the linear regression method (chapter 3, equation (3.8) of \href{{https://www.springer.com/gp/book/9780387848570}}{Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, The Elements of Statistical Learning, Springer}) is given as 

\[
\mathrm{Var}(\hat{\beta}) = \left(\hat{X}^T\hat{X}\right)^{-1}\sigma^2,
\]
with 
\[
\sigma^2 = \frac{1}{N-p-1}\sum_{i=1}^{N} (y_i-\tilde{y}_i)^2,
\]
where we have assumed that we fit a function of degree $p-1$ (for example a polynomial in $x$). 


% ------------------- end of main content ---------------

\end{document}

